# 研究进展

### 2024.01.21

16:04

我们发现将b的步长hd控制在1e-2时，误差反弹现象消失了；而b的步长hd在5e-4时，误差反弹现象明显，虽然高ReLU power时可以延缓反弹、增加精度，但前者进行500多次迭代都不会反弹。

我们初步预测，这是因为迭代次数高时，argmax会重复选择字典中的神经网络基函数，使得Pn中线性系统A奇异，算法

```
C_g = lsqminnorm(A(1:2*i,1:2*i),rhs_g(1:2*i));
```

将会按照一定规则给出C_g，即un_1=神经网络基函数线性组合的系数。

并且该规则在**字典小**时，给出的系数可以让高次迭代增加的神经网络基函数线性组合相互抵消；在**字典大**时，给出的系数无法让新选择的抵消，从而造成误差反弹。

但是为何反直觉地在字典小时误差不会反弹，仍需探究。
